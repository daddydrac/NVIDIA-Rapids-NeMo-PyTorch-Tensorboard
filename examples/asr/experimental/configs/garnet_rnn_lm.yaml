model: "RNN-LM"

optimization:
    batch_size: &bs 32  # >=256 have lower utilization and higher conv. time
    optimizer: "novograd"
    smoothing_coef: 0.0  # is it even working for chars?
    warmup_epochs: 1
    min_lr: 1e-5
    params:
        num_epochs: 25
        lr: 0.02
        weight_decay: 1e-5
        larc: false
        larc_eta: 1e-3
        luc: false
        luc_eta: 1e-3

target:
    labels: [
        " ", "'",
        "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m",
        "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z"
    ]

inference:
    batch_size: *bs

DecoderRNN:
    hidden_size: 1024
    attention_method: "general"
    attention_type: "none"
    in_dropout: 0.2
    gru_dropout: 0.2
    teacher_forcing: 0.6
    curriculum_learning: 0.75
    rnn_type: "gru"
    n_layers: 4
    tie_emb_out_weights: true