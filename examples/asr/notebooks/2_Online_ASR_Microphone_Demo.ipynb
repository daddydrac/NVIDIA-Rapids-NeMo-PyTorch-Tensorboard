{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates automatic speech recognition (ASR) from a microphone's stream in NeMo.\n",
    "\n",
    "It is **not a recommended** way to do inference in production workflows. If you are interested in \n",
    "production-level inference using NeMo ASR models, please sign-up to Jarvis early access program: https://developer.nvidia.com/nvidia-jarvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook requires PyAudio library to get a signal from an audio device.\n",
    "For Ubuntu, please run the following commands to install it:\n",
    "```\n",
    "sudo apt-get install -y portaudio19-dev\n",
    "pip install pyaudio\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.helpers import post_process_predictions\n",
    "import numpy as np\n",
    "import pyaudio as pa\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the checkpoints are available from NGC: https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5\n",
    "MODEL_YAML = '../configs/quartznet15x5.yaml'\n",
    "CHECKPOINT_ENCODER = '../../../quartznet15x5/JasperEncoder-STEP-247400.pt'\n",
    "CHECKPOINT_DECODER = '../../../quartznet15x5/JasperDecoderForCTC-STEP-247400.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "yaml = YAML(typ=\"safe\")\n",
    "with open(MODEL_YAML) as f:\n",
    "    model_definition = yaml.load(f)\n",
    "\n",
    "# some changes for streaming scenario\n",
    "model_definition['AudioToMelSpectrogramPreprocessor']['dither'] = 0\n",
    "model_definition['AudioToMelSpectrogramPreprocessor']['pad_to'] = 0\n",
    "# spectrogram normalization constants\n",
    "normalization = {}\n",
    "normalization['fixed_mean'] = [\n",
    "     -14.95827016, -12.71798736, -11.76067913, -10.83311182,\n",
    "     -10.6746914,  -10.15163465, -10.05378331, -9.53918999,\n",
    "     -9.41858904,  -9.23382904,  -9.46470918,  -9.56037,\n",
    "     -9.57434245,  -9.47498732,  -9.7635205,   -10.08113074,\n",
    "     -10.05454561, -9.81112681,  -9.68673603,  -9.83652977,\n",
    "     -9.90046248,  -9.85404766,  -9.92560366,  -9.95440354,\n",
    "     -10.17162966, -9.90102482,  -9.47471025,  -9.54416855,\n",
    "     -10.07109475, -9.98249912,  -9.74359465,  -9.55632283,\n",
    "     -9.23399915,  -9.36487649,  -9.81791084,  -9.56799225,\n",
    "     -9.70630899,  -9.85148006,  -9.8594418,   -10.01378735,\n",
    "     -9.98505315,  -9.62016094,  -10.342285,   -10.41070709,\n",
    "     -10.10687659, -10.14536695, -10.30828702, -10.23542833,\n",
    "     -10.88546868, -11.31723646, -11.46087382, -11.54877829,\n",
    "     -11.62400934, -11.92190509, -12.14063815, -11.65130117,\n",
    "     -11.58308531, -12.22214663, -12.42927197, -12.58039805,\n",
    "     -13.10098969, -13.14345864, -13.31835645, -14.47345634]\n",
    "normalization['fixed_std'] = [\n",
    "     3.81402054, 4.12647781, 4.05007065, 3.87790987,\n",
    "     3.74721178, 3.68377423, 3.69344,    3.54001005,\n",
    "     3.59530412, 3.63752368, 3.62826417, 3.56488469,\n",
    "     3.53740577, 3.68313898, 3.67138151, 3.55707266,\n",
    "     3.54919572, 3.55721289, 3.56723346, 3.46029304,\n",
    "     3.44119672, 3.49030548, 3.39328435, 3.28244406,\n",
    "     3.28001423, 3.26744937, 3.46692348, 3.35378948,\n",
    "     2.96330901, 2.97663111, 3.04575148, 2.89717604,\n",
    "     2.95659301, 2.90181116, 2.7111687,  2.93041291,\n",
    "     2.86647897, 2.73473181, 2.71495654, 2.75543763,\n",
    "     2.79174615, 2.96076456, 2.57376336, 2.68789782,\n",
    "     2.90930817, 2.90412004, 2.76187531, 2.89905006,\n",
    "     2.65896173, 2.81032176, 2.87769857, 2.84665271,\n",
    "     2.80863137, 2.80707634, 2.83752184, 3.01914511,\n",
    "     2.92046439, 2.78461139, 2.90034605, 2.94599508,\n",
    "     2.99099718, 3.0167554,  3.04649716, 2.94116777]\n",
    "model_definition['AudioToMelSpectrogramPreprocessor']['normalize'] = normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_factory = nemo.core.NeuralModuleFactory(\n",
    "    placement=nemo.core.DeviceType.GPU,\n",
    "    backend=nemo.core.Backend.PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.backends.pytorch.nm import DataLayerNM\n",
    "from nemo.core.neural_types import NeuralType, AudioSignal, LengthsType\n",
    "import torch\n",
    "\n",
    "# simple data layer to pass audio signal\n",
    "class AudioDataLayer(DataLayerNM):\n",
    "    @property\n",
    "    def output_ports(self):\n",
    "        return {\n",
    "            'audio_signal': NeuralType(('B', 'T'), AudioSignal(freq=self._sample_rate)),\n",
    "            'a_sig_length': NeuralType(tuple('B'), LengthsType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, sample_rate):\n",
    "        super().__init__()\n",
    "        self._sample_rate = sample_rate\n",
    "        self.output = True\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if not self.output:\n",
    "            raise StopIteration\n",
    "        self.output = False\n",
    "        return torch.as_tensor(self.signal, dtype=torch.float32), \\\n",
    "               torch.as_tensor(self.signal_shape, dtype=torch.int64)\n",
    "        \n",
    "    def set_signal(self, signal):\n",
    "        self.signal = np.reshape(signal.astype(np.float32)/32768., [1, -1])\n",
    "        self.signal_shape = np.expand_dims(self.signal.size, 0).astype(np.int64)\n",
    "        self.output = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def data_iterator(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate necessary neural modules\n",
    "data_layer = AudioDataLayer(sample_rate=model_definition['sample_rate'])\n",
    "\n",
    "data_preprocessor = nemo_asr.AudioToMelSpectrogramPreprocessor(\n",
    "    **model_definition['AudioToMelSpectrogramPreprocessor'])\n",
    "\n",
    "jasper_encoder = nemo_asr.JasperEncoder(\n",
    "    feat_in=model_definition['AudioToMelSpectrogramPreprocessor']['features'],\n",
    "    **model_definition['JasperEncoder'])\n",
    "\n",
    "jasper_decoder = nemo_asr.JasperDecoderForCTC(\n",
    "    feat_in=model_definition['JasperEncoder']['jasper'][-1]['filters'],\n",
    "    num_classes=len(model_definition['labels']))\n",
    "\n",
    "greedy_decoder = nemo_asr.GreedyCTCDecoder()\n",
    "\n",
    "# load pre-trained model\n",
    "jasper_encoder.restore_from(CHECKPOINT_ENCODER)\n",
    "jasper_decoder.restore_from(CHECKPOINT_DECODER)\n",
    "\n",
    "# Define inference DAG\n",
    "audio_signal, audio_signal_len = data_layer()\n",
    "processed_signal, processed_signal_len = data_preprocessor(\n",
    "    input_signal=audio_signal,\n",
    "    length=audio_signal_len)\n",
    "encoded, encoded_len = jasper_encoder(audio_signal=processed_signal,\n",
    "                                      length=processed_signal_len)\n",
    "log_probs = jasper_decoder(encoder_output=encoded)\n",
    "predictions = greedy_decoder(log_probs=log_probs)\n",
    "\n",
    "# inference method for audio signal (single instance)\n",
    "def infer_signal(self, signal):\n",
    "    data_layer.set_signal(signal)\n",
    "    tensors = self.infer([log_probs], verbose=False)\n",
    "    logits = tensors[0][0]\n",
    "    return logits\n",
    "\n",
    "neural_factory.infer_signal = infer_signal.__get__(neural_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for streaming frame-based ASR\n",
    "# 1) use reset() method to reset FrameASR's state\n",
    "# 2) call transcribe(frame) to do ASR on\n",
    "#    contiguous signal's frames\n",
    "class FrameASR:\n",
    "    \n",
    "    def __init__(self, neural_factory, model_definition,\n",
    "                 frame_len=2, frame_overlap=2.5, \n",
    "                 offset=10):\n",
    "        '''\n",
    "        Args:\n",
    "          frame_len: frame's duration, seconds\n",
    "          frame_overlap: duration of overlaps before and after current frame, seconds\n",
    "          offset: number of symbols to drop for smooth streaming\n",
    "        '''\n",
    "        self.vocab = list(model_definition['labels'])\n",
    "        self.vocab.append('_')\n",
    "        \n",
    "        self.sr = model_definition['sample_rate']\n",
    "        self.frame_len = frame_len\n",
    "        self.n_frame_len = int(frame_len * self.sr)\n",
    "        self.frame_overlap = frame_overlap\n",
    "        self.n_frame_overlap = int(frame_overlap * self.sr)\n",
    "        timestep_duration = model_definition['AudioToMelSpectrogramPreprocessor']['window_stride']\n",
    "        for block in model_definition['JasperEncoder']['jasper']:\n",
    "            timestep_duration *= block['stride'][0] ** block['repeat']\n",
    "        self.n_timesteps_overlap = int(frame_overlap / timestep_duration) - 2\n",
    "        self.buffer = np.zeros(shape=2*self.n_frame_overlap + self.n_frame_len,\n",
    "                               dtype=np.float32)\n",
    "        self.offset = offset\n",
    "        self.reset()\n",
    "        \n",
    "    def _decode(self, frame, offset=0):\n",
    "        assert len(frame)==self.n_frame_len\n",
    "        self.buffer[:-self.n_frame_len] = self.buffer[self.n_frame_len:]\n",
    "        self.buffer[-self.n_frame_len:] = frame\n",
    "        logits = neural_factory.infer_signal(self.buffer).cpu().numpy()[0]\n",
    "        # print(logits.shape)\n",
    "        decoded = self._greedy_decoder(\n",
    "            logits[self.n_timesteps_overlap:-self.n_timesteps_overlap], \n",
    "            self.vocab\n",
    "        )\n",
    "        return decoded[:len(decoded)-offset]\n",
    "    \n",
    "    def transcribe(self, frame=None, merge=True):\n",
    "        if frame is None:\n",
    "            frame = np.zeros(shape=self.n_frame_len, dtype=np.float32)\n",
    "        if len(frame) < self.n_frame_len:\n",
    "            frame = np.pad(frame, [0, self.n_frame_len - len(frame)], 'constant')\n",
    "        unmerged = self._decode(frame, self.offset)\n",
    "        if not merge:\n",
    "            return unmerged\n",
    "        return self.greedy_merge(unmerged)\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset frame_history and decoder's state\n",
    "        '''\n",
    "        self.buffer=np.zeros(shape=self.buffer.shape, dtype=np.float32)\n",
    "        self.prev_char = ''\n",
    "\n",
    "    @staticmethod\n",
    "    def _greedy_decoder(logits, vocab):\n",
    "        s = ''\n",
    "        for i in range(logits.shape[0]):\n",
    "            s += vocab[np.argmax(logits[i])]\n",
    "        return s\n",
    "\n",
    "    def greedy_merge(self, s):\n",
    "        s_merged = ''\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            if s[i] != self.prev_char:\n",
    "                self.prev_char = s[i]\n",
    "                if self.prev_char != '_':\n",
    "                    s_merged += self.prev_char\n",
    "        return s_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration of signal frame, seconds\n",
    "FRAME_LEN = 0.5\n",
    "# number of audio channels (expect mono signal)\n",
    "CHANNELS = 1\n",
    "# sample rate, Hz\n",
    "RATE = 16000\n",
    "\n",
    "CHUNK_SIZE = int(FRAME_LEN*RATE)\n",
    "asr = FrameASR(neural_factory, model_definition,\n",
    "               frame_len=FRAME_LEN, frame_overlap=2.0, \n",
    "               offset=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pa.PyAudio()\n",
    "print('Available audio input devices:')\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    if dev.get('maxInputChannels'):\n",
    "        print(i, dev.get('name'))\n",
    "print('Please type input device ID:')\n",
    "dev_idx = int(input())\n",
    "\n",
    "empty_counter = 0\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    global empty_counter\n",
    "    signal = np.frombuffer(in_data, dtype=np.int16)\n",
    "    text = asr.transcribe(signal)\n",
    "    if len(text):\n",
    "        print(text,end='')\n",
    "        empty_counter = 3\n",
    "    elif empty_counter > 0:\n",
    "        empty_counter -= 1\n",
    "        if empty_counter == 0:\n",
    "            print(' ',end='')\n",
    "    return (in_data, pa.paContinue)\n",
    "\n",
    "stream = p.open(format=pa.paInt16,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=dev_idx,\n",
    "                stream_callback=callback,\n",
    "                frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "print('Listening...')\n",
    "\n",
    "stream.start_stream()\n",
    "\n",
    "while stream.is_active():\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
